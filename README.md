[![PyPI version](https://badge.fury.io/py/nerpy.svg)](https://badge.fury.io/py/nerpy)
[![Downloads](https://pepy.tech/badge/nerpy)](https://pepy.tech/project/nerpy)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![GitHub contributors](https://img.shields.io/github/contributors/shibing624/nerpy.svg)](https://github.com/shibing624/nerpy/graphs/contributors)
[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)
[![GitHub issues](https://img.shields.io/github/issues/shibing624/nerpy.svg)](https://github.com/shibing624/nerpy/issues)
[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)

# NERpy
ğŸŒˆ Implementation of Named Entity Recognition using Python. 

**nerpy**å®ç°äº†BertSoftmaxã€BertCrfã€BertSpanç­‰å¤šç§å‘½åå®ä½“è¯†åˆ«æ¨¡å‹ï¼Œå¹¶åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šæ¯”è¾ƒäº†å„æ¨¡å‹çš„æ•ˆæœã€‚


**Guide**
- [Feature](#Feature)
- [Evaluation](#Evaluation)
- [Install](#install)
- [Usage](#usage)
- [Contact](#Contact)
- [Reference](#reference)


# Feature
### å‘½åå®ä½“è¯†åˆ«æ¨¡å‹
- [BertSoftmax](nerpy/ner_model.py)ï¼šBertSoftmaxåŸºäºBERTé¢„è®­ç»ƒæ¨¡å‹å®ç°å®ä½“è¯†åˆ«ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†BertSoftmaxæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
- [BertSpan](nerpy/bertspan.py)ï¼šBertSpanåŸºäºBERTè®­ç»ƒspanè¾¹ç•Œçš„è¡¨ç¤ºï¼Œæ¨¡å‹ç»“æ„æ›´é€‚é…å®ä½“è¾¹ç•Œè¯†åˆ«ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†BertSpanæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹

# Evaluation

### å®ä½“è¯†åˆ«

- è‹±æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š

| Arch | Backbone | Model Name | CoNLL-2003 | QPS |
| :-- | :--- | :--- | :-: | :--: |
| BertSoftmax | bert-base-uncased | bert4ner-base-uncased | 90.43 | 235 | 
| BertSoftmax | bert-base-cased | bert4ner-base-cased | 91.17 | 235 | 

- ä¸­æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š

| Arch | Backbone | Model Name | CNER | PEOPLE | Avg | QPS |
| :-- | :--- | :--- | :-: | :-: | :-: | :-: |
| BertSoftmax | bert-base-chinese | bert4ner-base-chinese | 94.98 | 95.25 | 95.12 | 222 |
| BertSpan | bert-base-chinese | bertspan4ner-base-chinese | 96.03 | 96.06 | 96.04 | 254 |

- æœ¬é¡¹ç›®releaseæ¨¡å‹çš„å®ä½“è¯†åˆ«è¯„æµ‹ç»“æœï¼š

| Arch | Backbone | Model Name | CNER(zh) | PEOPLE(zh) | CoNLL-2003(en) | QPS |
| :-- | :--- | :---- | :-: | :-: | :-: | :-: |
| BertSpan | bert-base-chinese | shibing624/bertspan4ner-base-chinese | 96.03 | 96.06 | - | 254 |
| BertSoftmax | bert-base-chinese | shibing624/bert4ner-base-chinese | 94.98 | 95.25 | - | 222 |
| BertSoftmax | bert-base-uncased | shibing624/bert4ner-base-uncased | - | - | 90.43 | 243 |

è¯´æ˜ï¼š
- ç»“æœå€¼å‡ä½¿ç”¨F1
- ç»“æœå‡åªç”¨è¯¥æ•°æ®é›†çš„trainè®­ç»ƒï¼Œåœ¨testä¸Šè¯„ä¼°å¾—åˆ°çš„è¡¨ç°ï¼Œæ²¡ç”¨å¤–éƒ¨æ•°æ®
- `shibing624/bertspan4ner-base-chinese`æ¨¡å‹è¾¾åˆ°baseçº§åˆ«é‡ŒSOTAæ•ˆæœï¼Œæ˜¯ç”¨BertSpanæ–¹æ³•è®­ç»ƒçš„ï¼Œ
 è¿è¡Œ[examples/training_bertspan_zh_demo.py](examples/training_bertspan_zh_demo.py)ä»£ç å¯åœ¨å„ä¸­æ–‡æ•°æ®é›†å¤ç°ç»“æœ
- `shibing624/bert4ner-base-chinese`æ¨¡å‹è¾¾åˆ°baseçº§åˆ«é‡Œè¾ƒå¥½æ•ˆæœï¼Œæ˜¯ç”¨BertSoftmaxæ–¹æ³•è®­ç»ƒçš„ï¼Œ
 è¿è¡Œ[examples/training_ner_model_zh_demo.py](examples/training_ner_model_zh_demo.py)ä»£ç å¯åœ¨å„ä¸­æ–‡æ•°æ®é›†å¤ç°ç»“æœ
- `shibing624/bert4ner-base-uncased`æ¨¡å‹æ˜¯ç”¨BertSoftmaxæ–¹æ³•è®­ç»ƒçš„ï¼Œ
 è¿è¡Œ[examples/training_ner_model_en_demo.py](examples/training_ner_model_en_demo.py)ä»£ç å¯åœ¨CoNLL-2003è‹±æ–‡æ•°æ®é›†å¤ç°ç»“æœ
- å„é¢„è®­ç»ƒæ¨¡å‹å‡å¯ä»¥é€šè¿‡transformersè°ƒç”¨ï¼Œå¦‚ä¸­æ–‡BERTæ¨¡å‹ï¼š`--model_name bert-base-chinese`
- ä¸­æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†ä¸‹è½½[é“¾æ¥è§ä¸‹æ–¹](#æ•°æ®é›†)
- QPSçš„GPUæµ‹è¯•ç¯å¢ƒæ˜¯Tesla V100ï¼Œæ˜¾å­˜32GB

# Demo

Demo: https://huggingface.co/spaces/shibing624/nerpy

![](docs/hf.png)

run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:
```shell
python examples/gradio_demo.py
```

 
# Install
python 3.8+

```shell
pip install torch # conda install pytorch
pip install -U nerpy
```

or

```shell
pip install torch # conda install pytorch
pip install -r requirements.txt

git clone https://github.com/shibing624/nerpy.git
cd nerpy
pip install --no-deps .
```

# Usage

## å‘½åå®ä½“è¯†åˆ«

#### è‹±æ–‡å®ä½“è¯†åˆ«ï¼š

```shell
>>> from nerpy import NERModel
>>> model = NERModel("bert", "shibing624/bert4ner-base-uncased")
>>> predictions, raw_outputs, entities = model.predict(["AL-AIN, United Arab Emirates 1996-12-06"], split_on_space=True)
entities:  [('AL-AIN,', 'LOC'), ('United Arab Emirates', 'LOC')]
```

#### ä¸­æ–‡å®ä½“è¯†åˆ«ï¼š

```shell
>>> from nerpy import NERModel
>>> model = NERModel("bert", "shibing624/bert4ner-base-chinese")
>>> predictions, raw_outputs, entities = model.predict(["å¸¸å»ºè‰¯ï¼Œç”·ï¼Œ1963å¹´å‡ºç”Ÿï¼Œå·¥ç§‘å­¦å£«ï¼Œé«˜çº§å·¥ç¨‹å¸ˆ"], split_on_space=False)
entities: [('å¸¸å»ºè‰¯', 'PER'), ('1963å¹´', 'TIME')]
```

example: [examples/base_zh_demo.py](examples/base_zh_demo.py)

```python
import sys

sys.path.append('..')
from nerpy import NERModel

if __name__ == '__main__':
    # BertSoftmaxä¸­æ–‡å®ä½“è¯†åˆ«æ¨¡å‹: NERModel("bert", "shibing624/bert4ner-base-chinese")
    # BertSpanä¸­æ–‡å®ä½“è¯†åˆ«æ¨¡å‹: NERModel("bertspan", "shibing624/bertspan4ner-base-chinese")
    model = NERModel("bert", "shibing624/bert4ner-base-chinese")
    sentences = [
        "å¸¸å»ºè‰¯ï¼Œç”·ï¼Œ1963å¹´å‡ºç”Ÿï¼Œå·¥ç§‘å­¦å£«ï¼Œé«˜çº§å·¥ç¨‹å¸ˆï¼ŒåŒ—äº¬ç‰©èµ„å­¦é™¢å®¢åº§å‰¯æ•™æˆ",
        "1985å¹´8æœˆ-1993å¹´åœ¨å›½å®¶ç‰©èµ„å±€ã€ç‰©èµ„éƒ¨ã€å›½å†…è´¸æ˜“éƒ¨é‡‘å±ææ–™æµé€šå¸ä»äº‹å›½å®¶ç»Ÿé…é’¢æä¸­ç‰¹ç§é’¢æå“ç§çš„è°ƒæ‹¨åˆ†é…å·¥ä½œï¼Œå…ˆåä»»ç§‘å‘˜ã€ä¸»ä»»ç§‘å‘˜ã€‚"
    ]
    predictions, raw_outputs, entities = model.predict(sentences)
    print(entities)
```

output:
```
[('å¸¸å»ºè‰¯', 'PER'), ('1963å¹´', 'TIME'), ('åŒ—äº¬ç‰©èµ„å­¦é™¢', 'ORG')]
[('1985å¹´', 'TIME'), ('8æœˆ', 'TIME'), ('1993å¹´', 'TIME'), ('å›½å®¶ç‰©èµ„å±€', 'ORG'), ('ç‰©èµ„éƒ¨', 'ORG'), ('å›½å†…è´¸æ˜“éƒ¨é‡‘å±ææ–™æµé€šå¸', 'ORG')]
```

- `shibing624/bert4ner-base-chinese`æ¨¡å‹æ˜¯BertSoftmaxæ–¹æ³•åœ¨ä¸­æ–‡PEOPLE(äººæ°‘æ—¥æŠ¥)æ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„ï¼Œæ¨¡å‹å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„
æ¨¡å‹åº“[shibing624/bert4ner-base-chinese](https://huggingface.co/shibing624/bert4ner-base-chinese)ï¼Œ
æ˜¯`nerpy.NERModel`æŒ‡å®šçš„é»˜è®¤æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡ä¸Šé¢ç¤ºä¾‹è°ƒç”¨ï¼Œæˆ–è€…å¦‚ä¸‹æ‰€ç¤ºç”¨[transformersåº“](https://github.com/huggingface/transformers)è°ƒç”¨ï¼Œ
æ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.cache/huggingface/transformers`
- `shibing624/bertspan4ner-base-chinese`æ¨¡å‹æ˜¯BertSpanæ–¹æ³•åœ¨ä¸­æ–‡PEOPLE(äººæ°‘æ—¥æŠ¥)æ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„ï¼Œæ¨¡å‹å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„
æ¨¡å‹åº“[shibing624/bertspan4ner-base-chinese](https://huggingface.co/shibing624/bertspan4ner-base-chinese)


#### Usage (HuggingFace Transformers)
Without [nerpy](https://github.com/shibing624/nerpy), you can use the model like this: 

First, you pass your input through the transformer model, then you have to apply the bio tag to get the entity words.

example: [examples/predict_use_origin_transformers_zh_demo.py](examples/predict_use_origin_transformers_zh_demo.py)

```python
import os
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
from seqeval.metrics.sequence_labeling import get_entities

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained("shibing624/bert4ner-base-chinese")
model = AutoModelForTokenClassification.from_pretrained("shibing624/bert4ner-base-chinese")
label_list = ['I-ORG', 'B-LOC', 'O', 'B-ORG', 'I-LOC', 'I-PER', 'B-TIME', 'I-TIME', 'B-PER']

sentence = "ç‹å®ä¼Ÿæ¥è‡ªåŒ—äº¬ï¼Œæ˜¯ä¸ªè­¦å¯Ÿï¼Œå–œæ¬¢å»ç‹åºœäº•æ¸¸ç©å„¿ã€‚"


def get_entity(sentence):
    tokens = tokenizer.tokenize(sentence)
    inputs = tokenizer.encode(sentence, return_tensors="pt")
    with torch.no_grad():
        outputs = model(inputs).logits
    predictions = torch.argmax(outputs, dim=2)
    char_tags = [(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())][1:-1]
    print(sentence)
    print(char_tags)

    pred_labels = [i[1] for i in char_tags]
    entities = []
    line_entities = get_entities(pred_labels)
    for i in line_entities:
        word = sentence[i[1]: i[2] + 1]
        entity_type = i[0]
        entities.append((word, entity_type))

    print("Sentence entity:")
    print(entities)


get_entity(sentence)
```
output:
```shell
ç‹å®ä¼Ÿæ¥è‡ªåŒ—äº¬ï¼Œæ˜¯ä¸ªè­¦å¯Ÿï¼Œå–œæ¬¢å»ç‹åºœäº•æ¸¸ç©å„¿ã€‚
[('ç‹', 'B-PER'), ('å®', 'I-PER'), ('ä¼Ÿ', 'I-PER'), ('æ¥', 'O'), ('è‡ª', 'O'), ('åŒ—', 'B-LOC'), ('äº¬', 'I-LOC'), ('ï¼Œ', 'O'), ('æ˜¯', 'O'), ('ä¸ª', 'O'), ('è­¦', 'O'), ('å¯Ÿ', 'O'), ('ï¼Œ', 'O'), ('å–œ', 'O'), ('æ¬¢', 'O'), ('å»', 'O'), ('ç‹', 'B-LOC'), ('åºœ', 'I-LOC'), ('äº•', 'I-LOC'), ('æ¸¸', 'O'), ('ç©', 'O'), ('å„¿', 'O'), ('ã€‚', 'O')]
Sentence entity:
[('ç‹å®ä¼Ÿ', 'PER'), ('åŒ—äº¬', 'LOC'), ('ç‹åºœäº•', 'LOC')]
```

### æ•°æ®é›†

#### å®ä½“è¯†åˆ«æ•°æ®é›†


| æ•°æ®é›† | è¯­æ–™ | ä¸‹è½½é“¾æ¥ | æ–‡ä»¶å¤§å° |
| :------- | :--------- | :---------: | :---------: |
| **`CNERä¸­æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†`** | CNER(12ä¸‡å­—) | [CNER github](https://github.com/shibing624/nerpy/tree/main/examples/data/cner)| 1.1MB |
| **`PEOPLEä¸­æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†`** | äººæ°‘æ—¥æŠ¥æ•°æ®é›†ï¼ˆ200ä¸‡å­—ï¼‰ | [PEOPLE github](https://github.com/shibing624/nerpy/tree/main/examples/data/people)| 12.8MB |
| **`CoNLL03è‹±æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†`** | CoNLL-2003æ•°æ®é›†ï¼ˆ22ä¸‡å­—ï¼‰ | [CoNLL03 github](https://github.com/shibing624/nerpy/tree/main/examples/data/conll03)| 1.7MB |

CNERä¸­æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†ï¼Œæ•°æ®æ ¼å¼ï¼š

```text
ç¾	B-LOC
å›½	I-LOC
çš„	O
å	B-PER
è±	I-PER
å£«	I-PER

æˆ‘	O
è·Ÿ	O
ä»–	O
```


## BertSoftmax æ¨¡å‹

BertSoftmaxå®ä½“è¯†åˆ«æ¨¡å‹ï¼ŒåŸºäºBERTçš„æ ‡å‡†åºåˆ—æ ‡æ³¨æ–¹æ³•ï¼š

Network structure:


<img src="docs/bert.png" width="500" />


æ¨¡å‹æ–‡ä»¶ç»„æˆï¼š
```
shibing624/bert4ner-base-chinese
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model_args.json
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ special_tokens_map.json
    â”œâ”€â”€ tokenizer_config.json
    â””â”€â”€ vocab.txt
```

#### BertSoftmax æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹

training example: [examples/training_ner_model_toy_demo.py](examples/bert_softmax_demo.py)


```python
import sys
import pandas as pd

sys.path.append('..')
from nerpy.ner_model import NERModel


# Creating samples
train_samples = [
    [0, "HuggingFace", "B-MISC"],
    [0, "Transformers", "I-MISC"],
    [0, "started", "O"],
    [0, "with", "O"],
    [0, "text", "O"],
    [0, "classification", "B-MISC"],
    [1, "Nerpy", "B-MISC"],
    [1, "Model", "I-MISC"],
    [1, "can", "O"],
    [1, "now", "O"],
    [1, "perform", "O"],
    [1, "NER", "B-MISC"],
]
train_data = pd.DataFrame(train_samples, columns=["sentence_id", "words", "labels"])

test_samples = [
    [0, "HuggingFace", "B-MISC"],
    [0, "Transformers", "I-MISC"],
    [0, "was", "O"],
    [0, "built", "O"],
    [0, "for", "O"],
    [0, "text", "O"],
    [0, "classification", "B-MISC"],
    [1, "Nerpy", "B-MISC"],
    [1, "Model", "I-MISC"],
    [1, "then", "O"],
    [1, "expanded", "O"],
    [1, "to", "O"],
    [1, "perform", "O"],
    [1, "NER", "B-MISC"],
]
test_data = pd.DataFrame(test_samples, columns=["sentence_id", "words", "labels"])

# Create a NERModel
model = NERModel(
    "bert",
    "bert-base-uncased",
    args={"overwrite_output_dir": True, "reprocess_input_data": True, "num_train_epochs": 1},
    use_cuda=False,
)

# Train the model
model.train_model(train_data)

# Evaluate the model
result, model_outputs, predictions = model.eval_model(test_data)
print(result, model_outputs, predictions)

# Predictions on text strings
sentences = ["Nerpy Model perform sentence NER", "HuggingFace Transformers build for text"]
predictions, raw_outputs, entities = model.predict(sentences, split_on_space=True)
print(predictions, entities)
```

- åœ¨ä¸­æ–‡CNERæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`BertSoftmax`æ¨¡å‹

example: [examples/training_ner_model_zh_demo.py](examples/training_ner_model_zh_demo.py)

```shell
cd examples
python training_ner_model_zh_demo.py --do_train --do_predict --num_epochs 5 --task_name cner
```
- åœ¨è‹±æ–‡CoNLL-2003æ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`BertSoftmax`æ¨¡å‹

example: [examples/training_ner_model_en_demo.py](examples/training_ner_model_en_demo.py)

```shell
cd examples
python training_ner_model_en_demo.py --do_train --do_predict --num_epochs 5
```


#### BertSpan æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹

- åœ¨ä¸­æ–‡CNERæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`BertSpan`æ¨¡å‹

example: [examples/training_bertspan_zh_demo.py](examples/training_bertspan_zh_demo.py)

```shell
cd examples
python training_bertspan_zh_demo.py --do_train --do_predict --num_epochs 5 --task_name cner
```

# Contact

- Issue(å»ºè®®)ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/nerpy.svg)](https://github.com/shibing624/nerpy/issues)
- é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
- å¾®ä¿¡æˆ‘ï¼š
åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸-NLP* è¿›NLPäº¤æµç¾¤ã€‚

<img src="docs/wechat.jpeg" width="200" />


# Citation

å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†nerpyï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š

APA:
```latex
Xu, M. nerpy: Named Entity Recognition Toolkit (Version 0.0.2) [Computer software]. https://github.com/shibing624/nerpy
```

BibTeX:
```latex
@software{Xu_nerpy_Text_to,
author = {Xu, Ming},
title = {{nerpy: Named Entity Recognition Toolkit}},
url = {https://github.com/shibing624/nerpy},
version = {0.0.2}
}
```

# License


æˆæƒåè®®ä¸º [The Apache License 2.0](LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ nerpyçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚


# Contribute
é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š

 - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
 - ä½¿ç”¨`python -m pytest -v`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„

ä¹‹åå³å¯æäº¤PRã€‚

# Reference
- [huggingface/transformers](https://github.com/huggingface/transformers)
