[![PyPI version](https://badge.fury.io/py/nerpy.svg)](https://badge.fury.io/py/nerpy)
[![Downloads](https://pepy.tech/badge/nerpy)](https://pepy.tech/project/nerpy)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![GitHub contributors](https://img.shields.io/github/contributors/shibing624/nerpy.svg)](https://github.com/shibing624/nerpy/graphs/contributors)
[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![python_version](https://img.shields.io/badge/Python-3.5%2B-green.svg)](requirements.txt)
[![GitHub issues](https://img.shields.io/github/issues/shibing624/nerpy.svg)](https://github.com/shibing624/nerpy/issues)
[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)

# NERpy
ğŸŒˆ Implementation of Named Entity Recognition using Python. 

**nerpy**å®ç°äº†Bert2Tagã€Bert2Spanç­‰å¤šç§å‘½åå®ä½“è¯†åˆ«æ¨¡å‹ï¼Œå¹¶åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šæ¯”è¾ƒäº†å„æ¨¡å‹çš„æ•ˆæœã€‚


**Guide**
- [Feature](#Feature)
- [Evaluation](#Evaluation)
- [Install](#install)
- [Usage](#usage)
- [Contact](#Contact)
- [Reference](#reference)


# Feature
### å‘½åå®ä½“è¯†åˆ«æ¨¡å‹
- [CoSENT(Cosine Sentence)](nerpy/cosent_model.py)ï¼šCoSENTæ¨¡å‹æå‡ºäº†ä¸€ç§æ’åºçš„æŸå¤±å‡½æ•°ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´è´´è¿‘é¢„æµ‹ï¼Œæ¨¡å‹æ”¶æ•›é€Ÿåº¦å’Œæ•ˆæœæ¯”Sentence-BERTæ›´å¥½ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†CoSENTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹

# Evaluation

### å®ä½“è¯†åˆ«

- è‹±æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š

| Arch | Backbone | Model Name | English-STS-B | 
| :-- | :--- | :--- | :-: |
| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68 |

- ä¸­æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š

| Arch | Backbone | Model Name | ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | QPS |
| :-- | :--- | :--- | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| SBERT | hfl/chinese-roberta-wwm-ext | SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | - |

- æœ¬é¡¹ç›®releaseæ¨¡å‹çš„ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœï¼š

| Arch | Backbone | Model Name | ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | QPS |
| :-- | :--- | :---- | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| Word2Vec | word2vec | w2v-light-tencent-chinese | 20.00 | 31.49 | 59.46 | 2.57 | 55.78 | 33.86 | 10283 |

è¯´æ˜ï¼š
- ç»“æœå€¼å‡ä½¿ç”¨F1
- ç»“æœå‡åªç”¨è¯¥æ•°æ®é›†çš„trainè®­ç»ƒï¼Œåœ¨testä¸Šè¯„ä¼°å¾—åˆ°çš„è¡¨ç°ï¼Œæ²¡ç”¨å¤–éƒ¨æ•°æ®
- `CoSENT-macbert-base`æ¨¡å‹è¾¾åˆ°åŒçº§åˆ«å‚æ•°é‡SOTAæ•ˆæœï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](examples/training_sup_text_matching_model.py)ä»£ç å¯åœ¨å„æ•°æ®é›†å¤ç°ç»“æœ
- å„é¢„è®­ç»ƒæ¨¡å‹å‡å¯ä»¥é€šè¿‡transformersè°ƒç”¨ï¼Œå¦‚MacBERTæ¨¡å‹ï¼š`--model_name hfl/chinese-macbert-base`
- ä¸­æ–‡åŒ¹é…æ•°æ®é›†ä¸‹è½½[é“¾æ¥è§ä¸‹æ–¹](#æ•°æ®é›†)
- ä¸­æ–‡åŒ¹é…ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œpoolingæœ€ä¼˜æ˜¯`first_last_avg`ï¼Œå³ SentenceModel çš„`EncoderType.FIRST_LAST_AVG`ï¼Œå…¶ä¸`EncoderType.MEAN`çš„æ–¹æ³•åœ¨é¢„æµ‹æ•ˆæœä¸Šå·®å¼‚å¾ˆå°
- QPSçš„GPUæµ‹è¯•ç¯å¢ƒæ˜¯Tesla V100ï¼Œæ˜¾å­˜32GB

# Demo

Official Demo: http://42.193.145.218/product/short_text_sim/

HuggingFace Demo: https://huggingface.co/spaces/shibing624/nerpy

![](docs/hf.png)

# Install
```
pip3 install torch # conda install pytorch
pip3 install -U nerpy
```

or

```
git clone https://github.com/shibing624/nerpy.git
cd nerpy
python3 setup.py install
```

### æ•°æ®é›†
ä¸­æ–‡å®ä½“è¯†åˆ«æ•°æ®é›†å·²ç»ä¸Šä¼ åˆ°huggingface datasets [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)

# Usage

## å®ä½“è¯†åˆ«

åŸºäº`pretrained model`è®¡ç®—å®ä½“è¯†åˆ«ï¼š

```shell
>>> from nerpy import Bert2Tag
>>> m = Bert2Tag()
>>> m.ner("University of California is located in California, United States")
{'LOCATION': ['California', 'United States'], 'ORGANIZATION': ['University of California']}
```

example: [examples/ner_demo.py](examples/ner_demo.py)

```python
import sys

sys.path.append('..')
from nerpy import Bert2Tag

def compute_ner(model):
    sentences = [
        'åŒ—äº¬å¤§å­¦å­¦ç”Ÿæ¥åˆ°æ°´ç«‹æ–¹è§‚çœ‹æ°´ä¸ŠèŠ­è•¾è¡¨æ¼”',
        'University of California is located in California, United States'
    ]
    entities = model.ner(sentences)
    print(entities)


if __name__ == "__main__":
    # ä¸­æ–‡å®ä½“è¯†åˆ«æ¨¡å‹ï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
    t2v_model = Bert2Tag("shibing624/nerpy-base-chinese")
    compute_ner(t2v_model)

    # æ”¯æŒå¤šè¯­è¨€çš„å®ä½“è¯†åˆ«æ¨¡å‹ï¼Œè‹±æ–‡å®ä½“è¯†åˆ«ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
    sbert_model = Bert2Tag("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    compute_ner(sbert_model)
```

output:
```
{'LOCATION': ['æ°´ç«‹æ–¹'], 'ORGANIZATION': ['åŒ—äº¬å¤§å­¦']}
{'LOCATION': ['California', 'United States'], 'ORGANIZATION': ['University of California']}
```

- `shibing624/nerpy-base-chinese`æ¨¡å‹æ˜¯CoSENTæ–¹æ³•åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„ï¼Œæ¨¡å‹å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„
æ¨¡å‹åº“[shibing624/nerpy-base-chinese](https://huggingface.co/shibing624/nerpy-base-chinese)ï¼Œ
æ˜¯`nerpy.SentenceModel`æŒ‡å®šçš„é»˜è®¤æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡ä¸Šé¢ç¤ºä¾‹è°ƒç”¨ï¼Œæˆ–è€…å¦‚ä¸‹æ‰€ç¤ºç”¨[transformersåº“](https://github.com/huggingface/transformers)è°ƒç”¨ï¼Œ
æ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.cache/huggingface/transformers`

#### Usage (HuggingFace Transformers)
Without [nerpy](https://github.com/shibing624/nerpy), you can use the model like this: 

First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.

example: [examples/use_origin_transformers_demo.py](examples/use_origin_transformers_demo.py)

```python
import os
import torch
from transformers import AutoTokenizer, AutoModel

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


# Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained('shibing624/nerpy-base-chinese')
model = AutoModel.from_pretrained('shibing624/nerpy-base-chinese')
sentences = ['åŒ—äº¬å¤§å­¦å­¦ç”Ÿæ¥åˆ°æ°´ç«‹æ–¹è§‚çœ‹æ°´ä¸ŠèŠ­è•¾è¡¨æ¼”']
# Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)
print("Sentence Entities:")
print(model_output)
```



## Bert2Tag model

Sentence-BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œè¡¨å¾å¼å¥å‘é‡è¡¨ç¤ºæ–¹æ¡ˆ

Network structure:

Training:

<img src="docs/sbert_train.png" width="300" />


Inference:

<img src="docs/sbert_inference.png" width="300" />

#### Bert2Tag ç›‘ç£æ¨¡å‹
- åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`MacBERT+Bert2Tag`æ¨¡å‹

example: [examples/training_sup_text_matching_model.py](examples/training_sup_text_matching_model.py)

```shell
cd examples
python3 training_sup_text_matching_model.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-sbert
```
- åœ¨è‹±æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`BERT+SBERT`æ¨¡å‹

example: [examples/training_sup_text_matching_model_en.py](examples/training_sup_text_matching_model_en.py)

```shell
cd examples
python3 training_sup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-sbert
```


# Contact

- Issue(å»ºè®®)ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/nerpy.svg)](https://github.com/shibing624/nerpy/issues)
- é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
- å¾®ä¿¡æˆ‘ï¼š
åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸-NLP* è¿›NLPäº¤æµç¾¤ã€‚

<img src="docs/wechat.jpeg" width="200" />


# Citation

å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†nerpyï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š

APA:
```latex
Xu, M. nerpy: Text to vector toolkit (Version 0.0.2) [Computer software]. https://github.com/shibing624/nerpy
```

BibTeX:
```latex
@software{Xu_nerpy_Text_to,
author = {Xu, Ming},
title = {{nerpy: Named Entity Recognition Toolkit}},
url = {https://github.com/shibing624/nerpy},
version = {0.0.2}
}
```

# License


æˆæƒåè®®ä¸º [The Apache License 2.0](LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ nerpyçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚


# Contribute
é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š

 - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
 - ä½¿ç”¨`python setup.py test`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„

ä¹‹åå³å¯æäº¤PRã€‚

# Reference
- [transformers](https://github.com/huggingface/transformers)
